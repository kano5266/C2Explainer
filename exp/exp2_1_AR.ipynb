{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75750720-6a58-449b-8b50-ef4b89b5aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
      "PyTorch version: 2.0.1\n",
      "PyTorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from cf_explainer.gcn_conv import GCNConv\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
    "import torch_geometric\n",
    "from torch_geometric.explain import Explainer\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GINConv, GIN\n",
    "from cf_explainer import C2Explainer\n",
    "from cf_explainer.utils import seed_everything\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "'''Config parameters'''\n",
    "use_cuda_if_available = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and use_cuda_if_available else 'cpu')\n",
    "\n",
    "# seed_everything(42, deterministic=True)\n",
    "# if error when setting use_deterministic_algorithms(True)\n",
    "# try this:\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"PyTorch device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e638ff49-64f2-4968-9206-7b1bf8183505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def results(num_perturbs, prop_perturbs):\n",
    "    print(\"######\")\n",
    "    if len(num_perturbs) != 0:\n",
    "        size = sum(num_perturbs)/(2*len(num_perturbs))\n",
    "        prop = sum(prop_perturbs)/len(prop_perturbs)\n",
    "    else:\n",
    "        size = \"N/A\"\n",
    "        prop = \"N/A\"\n",
    "    print(f\"size: {size}, num_success: {len(num_perturbs)}, prop_perturbs: {prop}\")\n",
    "    print(\"finished\")\n",
    "    return size, prop\n",
    "\n",
    "\n",
    "def explain(model, data, explainer, seed=42):\n",
    "    seed_everything(seed, deterministic=True)\n",
    "    result = []\n",
    "    \n",
    "    condition = (data.test_mask.cpu() | data.val_mask.cpu())\n",
    "    df_cf = np.where(condition)[0].tolist()\n",
    "\n",
    "    print(len(df_cf))\n",
    "    \n",
    "    explainer = Explainer(\n",
    "        model=model,\n",
    "        algorithm=explainer,\n",
    "        explanation_type='model',\n",
    "        node_mask_type=None,\n",
    "        edge_mask_type='object',\n",
    "        model_config=dict(\n",
    "            mode='multiclass_classification',\n",
    "            task_level='node',\n",
    "            return_type='raw',\n",
    "        ), \n",
    "    )\n",
    "    \n",
    "    cfs = []\n",
    "    num_perturbs = []\n",
    "    prop_perturbs = []\n",
    "    indices = []\n",
    "\n",
    "    for i in tqdm(df_cf):\n",
    "        explanation = explainer(data.x, data.edge_index, index=i)\n",
    "        \n",
    "        if hasattr(explanation, \"perturbs\"):\n",
    "            if explanation.perturbs < 20:\n",
    "                cfs.append(explanation.cf)\n",
    "                num_perturbs.append(explanation.perturbs)\n",
    "                prop_perturbs.append(explanation.prop_perturbs)\n",
    "                indices.append(i)\n",
    "    \n",
    "    \n",
    "    size, prop = results(num_perturbs, prop_perturbs)\n",
    "    \n",
    "    result.append([len(df_cf), size, len(num_perturbs), prop])\n",
    "    print(f\"Fedility: {len(num_perturbs)/len(df_cf)}, Num_perturbs: {size}, Similarity: {1-prop}\")\n",
    "    return result, cfs, num_perturbs, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495415b-f156-4654-be64-3d879b975048",
   "metadata": {},
   "source": [
    "# LoanDecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c01a388-d22a-4356-a3e6-f09c57224bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymmt4090no1/miniconda3/envs/pyg/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 3950], num_nodes=1000, x=[1000, 2], y=[1000], train_mask=[1000], val_mask=[1000], test_mask=[1000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, nhid, nout, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(2, nhid, normalize=True)\n",
    "        self.conv2 = GCNConv(nhid, nhid, normalize=True)\n",
    "        self.conv3 = GCNConv(nhid, nout, normalize=True)\n",
    "        self.lin1 = Linear(nout, nout)\n",
    "        self.lin2 = Linear(nout,2)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "model = GCN(nhid=100, nout=20, dropout=0).to(device)\n",
    "model.load_state_dict(torch.load(\"../models/GCN_LoanDecision_sd.pt\", weights_only=True))\n",
    "\n",
    "with open(\"../data/LoanDecision.pickle\", \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc9c9fcf-f74a-4350-bd60-daf8258c4ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e31f0d8bfd46f3812450d000d1d8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######\n",
      "size: 0.9169603586196899, num_success: 198, prop_perturbs: 0.005527470260858536\n",
      "finished\n",
      "Fedility: 0.99, Num_perturbs: 0.9169603586196899, Similarity: 0.9944725036621094\n",
      "CPU times: user 20min 31s, sys: 1.24 s, total: 20min 32s\n",
      "Wall time: 20min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "explainer = C2Explainer(epochs=1000, lr=0.1, silent_mode=True, undirected=True, AR_mode=True, FPM=True)\n",
    "\n",
    "result1, cfs1, num_perturbs1, indices1 = explain(model, data, explainer, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2375ec-1ab1-43bc-b1ee-5f9d91d189c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033a0286e14b46e5b4d11e8af608501c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######\n",
      "size: 1.425414364640884, num_success: 181, prop_perturbs: 0.007844492793083191\n",
      "finished\n",
      "Fedility: 0.905, Num_perturbs: 1.425414364640884, Similarity: 0.9921554923057556\n",
      "CPU times: user 19min 55s, sys: 1.25 s, total: 19min 57s\n",
      "Wall time: 20min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "explainer = C2Explainer(epochs=1000, lr=0.1, silent_mode=True, undirected=True, AR_mode=True)\n",
    "\n",
    "result2, cfs2, num_perturbs2, indices2 = explain(model, data, explainer, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a35049-2899-458c-bc02-d2fc4bc1c3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f92db0d1f8041b6b36a21473f800dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######\n",
      "size: 1.2043010752688172, num_success: 186, prop_perturbs: 0.006940612103790045\n",
      "finished\n",
      "Fedility: 0.93, Num_perturbs: 1.2043010752688172, Similarity: 0.9930593967437744\n",
      "CPU times: user 46min 25s, sys: 1.43 s, total: 46min 26s\n",
      "Wall time: 46min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "explainer = C2Explainer(epochs=1000, lr=0.1, silent_mode=True, undirected=True, AR_mode=False)\n",
    "\n",
    "result3, cfs3, num_perturbs3, indices3 = explain(model, data, explainer, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecbfee-337c-42e2-adb9-dcfead607972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51bb3e-6fe2-4f7b-903c-4a60ed09ac9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89482773-d0f7-4d43-8cd5-11adcae5f848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.utils import to_undirected, coalesce\n",
    "\n",
    "def remove_edges(edge_index, edge_index_to_remove):\n",
    "        r\"\"\"\n",
    "        remove edges in edge_index that are also in edge_index_to_remove.\n",
    "        \"\"\"\n",
    "        # Trick from https://github.com/pyg-team/pytorch_geometric/discussions/9440\n",
    "        all_edge_index = torch.cat([edge_index,\n",
    "                                    edge_index_to_remove], dim=1)\n",
    "\n",
    "        # mark removed edges as 1 and 0 otherwise\n",
    "        all_edge_weights = torch.cat([torch.zeros(edge_index.size(1)),\n",
    "                                      torch.ones(edge_index_to_remove.size(1))]\n",
    "                                     ).to(all_edge_index.device)\n",
    "\n",
    "        all_edge_index, all_edge_weights = coalesce(\n",
    "            all_edge_index, all_edge_weights)\n",
    "\n",
    "        # remove edges indicated by 1\n",
    "        edge_index = all_edge_index[:, all_edge_weights == 0]\n",
    "        return edge_index\n",
    "\n",
    "def isAR(edge_index1, edge_index2, num_perturbs, index):\n",
    "    subset, _, _, hard_edge_mask1 = k_hop_subgraph(\n",
    "                index,\n",
    "                num_hops=3,\n",
    "                edge_index=edge_index1,\n",
    "                relabel_nodes=False)\n",
    "    \n",
    "    edge_index1 = edge_index1[:, hard_edge_mask1] # edges\n",
    "    \n",
    "    a = subset\n",
    "    b = torch.tensor([index]).to(subset.device)\n",
    "    AR_edge_index1 = torch.cartesian_prod(a, b).T.to(int)\n",
    "    AR_edge_index1 = to_undirected(AR_edge_index1)\n",
    "    AR_edge_index1 = remove_edges(AR_edge_index1, edge_index2)\n",
    "    \n",
    "    edges1 = edge_index1.t()\n",
    "    AR_edges1 = AR_edge_index1.t()\n",
    "    \n",
    "    # Find unique edges in edge_index1\n",
    "    unique_edges1 = torch.empty((0, 2), dtype=torch.long).to(edge_index1.device)\n",
    "    for edge in edges1:\n",
    "        if any((edge == AR_edges1).all(dim=1)):\n",
    "            unique_edges1 = torch.cat((unique_edges1, edge.unsqueeze(0)), dim=0)\n",
    "            \n",
    "            \n",
    "    #======#\n",
    "    \n",
    "    _, _, _, hard_edge_mask1 = k_hop_subgraph(\n",
    "                index,\n",
    "                num_hops=1,\n",
    "                edge_index=edge_index1,\n",
    "                relabel_nodes=False)\n",
    "\n",
    "    _, _, _, hard_edge_mask2 = k_hop_subgraph(\n",
    "                    index,\n",
    "                    num_hops=1,\n",
    "                    edge_index=edge_index2,\n",
    "                    relabel_nodes=False)\n",
    "    \n",
    "    edge_index1 = edge_index1[:, hard_edge_mask1] \n",
    "    edge_index2 = edge_index2[:, hard_edge_mask2]    \n",
    "    \n",
    "    edges1 = edge_index1.t()\n",
    "    edges2 = edge_index2.t()\n",
    "\n",
    "    # Find unique edges in edge_index2\n",
    "    unique_edges2 = torch.empty((0, 2), dtype=torch.long).to(edge_index2.device)\n",
    "    for edge in edges2:\n",
    "        if not any((edge == edges1).all(dim=1)):\n",
    "            unique_edges2 = torch.cat((unique_edges2, edge.unsqueeze(0)), dim=0)\n",
    "\n",
    "    # Transpose to get back to edge_index format\n",
    "    unique_edge_index1 = unique_edges1.t()\n",
    "    unique_edge_index2 = unique_edges2.t()\n",
    "    \n",
    "    if unique_edge_index1.size(1)==0 & unique_edge_index2.size(1)==0:\n",
    "        return 0\n",
    "    elif unique_edge_index1.size(1)+unique_edge_index2.size(1)==num_perturbs[i]: # all edges perturbed are in hop-1\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#     print(\"Edges in added:\")\n",
    "#     print(unique_edge_index1)\n",
    "\n",
    "#     print(\"Edges in deleted:\")\n",
    "#     print(unique_edge_index2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cefa580-fc61-46dd-8d16-e854a82030e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_AR=0\n",
    "# for i, index in enumerate(indices1):\n",
    "#     a = isAR(cfs1[i], data.edge_index, num_perturbs1, index) # isAR ii not suitable for counterfactuals with feature perturbations\n",
    "#     # print(a)\n",
    "#     num_AR+=a\n",
    "#     # print(\"======\")\n",
    "# print(\"num_AR\", num_AR)\n",
    "\n",
    "## isAR ii not suitable for counterfactuals with feature perturbations, we calculate the AR_val manually for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18eb5bc-7c82-4c0c-bb5b-8ae08fddc16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_AR 181\n",
      "AR_val 0.905\n"
     ]
    }
   ],
   "source": [
    "num_AR=0\n",
    "for i, index in enumerate(indices2):\n",
    "    a = isAR(cfs2[i], data.edge_index, num_perturbs2, index)\n",
    "    # print(a)\n",
    "    num_AR+=a\n",
    "    # print(\"======\")\n",
    "print(\"num_AR\", num_AR)\n",
    "print(\"AR_val\", num_AR/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5142f573-06ba-43c9-a0a0-c983ebbb13c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_AR 0.73\n"
     ]
    }
   ],
   "source": [
    "num_AR=0\n",
    "for i, index in enumerate(indices3):\n",
    "    a = isAR(cfs3[i], data.edge_index, num_perturbs3, index)\n",
    "    # print(a)\n",
    "    num_AR+=a\n",
    "    # print(\"======\")\n",
    "print(\"num_AR\", num_AR/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c3656-1608-4f4f-933c-37cec470baf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
